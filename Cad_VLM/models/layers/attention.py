import os
import sys
from typing import List, Optional, Tuple, Union, Dict, Any
import torch
from torch import Tensor
import torch.nn as nn
from torch.nn.parameter import Parameter
from torch.nn.init import xavier_normal_, xavier_uniform_, constant_

from .layer_utils import *
from .functional import multi_head_attention_forward


# TODO: Replace the MultiHeadAttention layer with the code written for CADLGen (Adding Flash and Memory efficient Attention)
class MultiHeadAttention(nn.Module):
    """Allows the model to jointly attend to information from different representation subspaces."""

    __annotations__ = {
        "bias_k": Optional[Tensor],
        "bias_v": Optional[Tensor],
        "in_proj_weight": Optional[Parameter],
        "q_proj_weight": Optional[Parameter],
        "k_proj_weight": Optional[Parameter],
        "v_proj_weight": Optional[Parameter],
        "in_proj_bias": Optional[Parameter],
        "out_proj": nn.Linear,
    }
    __constants__ = [
        "q_proj_weight",
        "k_proj_weight",
        "v_proj_weight",
        "in_proj_weight",
    ]

    def __init__(
        self,
        input_dim: int,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        bias: bool = True,
        add_bias_kv: bool = False,
        add_zero_attn: bool = False,
        kdim: Optional[int] = None,
        vdim: Optional[int] = None,
    ) -> None:
        super().__init__()
        self.input_dim = input_dim
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        assert (
            self.head_dim * num_heads == self.embed_dim
        ), "embed_dim must be divisible by num_heads"

        if not self._qkv_same_embed_dim:
            self.q_proj_weight = Parameter(torch.empty(embed_dim, embed_dim))
            self.k_proj_weight = Parameter(torch.empty(embed_dim, self.kdim))
            self.v_proj_weight = Parameter(torch.empty(embed_dim, self.vdim))
            self.register_parameter("in_proj_weight", None)
        else:
            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))
            self.register_parameter("q_proj_weight", None)
            self.register_parameter("k_proj_weight", None)
            self.register_parameter("v_proj_weight", None)

        if bias:
            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))
        else:
            self.register_parameter("in_proj_bias", None)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)

        if add_bias_kv:
            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))
            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))
        else:
            self.bias_k = None
            self.bias_v = None

        self.add_zero_attn = add_zero_attn
        self._reset_parameters()

    def _reset_parameters(self) -> None:
        if self._qkv_same_embed_dim:
            xavier_uniform_(self.in_proj_weight)
        else:
            xavier_uniform_(self.q_proj_weight)
            xavier_uniform_(self.k_proj_weight)
            xavier_uniform_(self.v_proj_weight)

        if self.in_proj_bias is not None:
            constant_(self.in_proj_bias, 0.0)
            constant_(self.out_proj.bias, 0.0)
        if self.bias_k is not None:
            xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            xavier_normal_(self.bias_v)

    def __setstate__(self, state):
        # Support loading old MultiheadAttention checkpoints generated by v1.1.0
        if "_qkv_same_embed_dim" not in state:
            state["_qkv_same_embed_dim"] = True

        super(MultiHeadAttention, self).__setstate__(state)

    def forward(
        self,
        query: Tensor,
        key: Tensor,
        value: Tensor,
        key_padding_mask: Optional[Tensor] = None,
        need_weights: bool = True,
        attn_mask: Optional[Tensor] = None,
        use_3d: bool = False,
    ) -> Tuple[Tensor, Optional[Tensor]]:
        """
        Args:
            query: Input tensor of shape (batch_size, n_samples, n_features)
            key: Input tensor of shape (batch_size, n_samples, n_features)
            value: Input tensor of shape (batch_size, n_samples, n_features)
            key_padding_mask: Binary mask for key padding elements
            need_weights: Whether to return attention weights
            attn_mask: 2D or 3D mask preventing attention to certain positions
            use_3d: Whether to use 3D attention mask
        Returns:
            Tuple of:
            - Output tensor of shape (batch_size, n_samples, embed_dim)
            - Optional attention weights tensor
        """
        query = query.transpose(0, 1)  # (N,B,E)
        key = key.transpose(0, 1)  # (N,B,E)
        value = value.transpose(0, 1)  # (N,B,E)

        # Attention mask is same for all batches
        if attn_mask is not None and attn_mask.dim() > 2 and not use_3d:
            attn_mask = attn_mask[0]  # (N,S)

        if not self._qkv_same_embed_dim:
            return multi_head_attention_forward(
                query,
                key,
                value,
                self.embed_dim,
                self.num_heads,
                self.in_proj_weight,
                self.in_proj_bias,
                self.bias_k,
                self.bias_v,
                self.add_zero_attn,
                self.dropout,
                self.out_proj.weight,
                self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
                use_separate_proj_weight=True,
                q_proj_weight=self.q_proj_weight,
                k_proj_weight=self.k_proj_weight,
                v_proj_weight=self.v_proj_weight,
            )
        else:
            return multi_head_attention_forward(
                query,
                key,
                value,
                self.embed_dim,
                self.num_heads,
                self.in_proj_weight,
                self.in_proj_bias,
                self.bias_k,
                self.bias_v,
                self.add_zero_attn,
                self.dropout,
                self.out_proj.weight,
                self.out_proj.bias,
                training=self.training,
                key_padding_mask=key_padding_mask,
                need_weights=need_weights,
                attn_mask=attn_mask,
            )

class CrossAttention(nn.Module):
    """Cross Attention Layer for multi-modal Feature sharing."""

    def __init__(
        self,
        input_dim_list: List[int],
        output_dim: int = 512,
        num_heads: int = 1,
        query_name: str = "pc",
        context_1_name: str = "cad",
        dropout: float = 0,
        block_level: int = 1,
    ) -> None:
        super().__init__()
        self.query_name = query_name
        self.context_1_name = context_1_name
        self.block_level = block_level

        self.mha = MultiHeadAttention(
            input_dim=input_dim_list[0],
            embed_dim=output_dim,
            dropout=dropout,
            num_heads=num_heads,
        )

    def forward(
        self,
        X: Tensor,
        Y: Tensor,
        Z: Tensor,
        attn_mask: Optional[Tensor] = None,
        key_padding_mask: Optional[Tensor] = None,
    ) -> Tuple[Tensor, Optional[Tensor]]:
        """
        Args:
            X: Query tensor
            Y: Key tensor
            Z: Value tensor
            attn_mask: Optional attention mask
            key_padding_mask: Optional key padding mask
        Returns:
            Tuple of:
            - Output tensor
            - Optional attention weights
        """
        return self.mha(X, Y, Z, key_padding_mask, True, attn_mask)

